import numpy as np

class NeuralNetwork:
    def __init__(self, layers, learning_rate=0.1):
        self.layers = layers
        self.learning_rate = learning_rate
        self.weights = []
        self.biases = []
        self.activation_functions = []

        # Initialize weights and biases
        for i in range(1, len(layers)):
            # Initialize weights with random values from a normal distribution
            self.weights.append(np.random.randn(layers[i], layers[i-1]))
            # Initialize biases as zeros
            self.biases.append(np.zeros((layers[i], 1)))
            # Use sigmoid activation function for all layers except the output layer
            if i < len(layers) - 1:
                self.activation_functions.append(self.sigmoid)
            else:
                self.activation_functions.append(self.softmax)  # softmax for output layer

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def softmax(self, x):
        exp_scores = np.exp(x - np.max(x))  # for numerical stability
        return exp_scores / np.sum(exp_scores, axis=0, keepdims=True)

    def forward_propagation(self, X):
        activations = []
        input = X.T
        activations.append(input)

        # Perform forward propagation
        for i in range(len(self.layers) - 1):
            output = np.dot(self.weights[i], input) + self.biases[i]
            activation = self.activation_functions[i](output)
            activations.append(activation)
            input = activation

        return activations

    def backward_propagation(self, X, y, activations):
        # Convert target labels y to one-hot encoding
        y_one_hot = np.zeros((self.layers[-1], X.shape[0]))
        y_one_hot[y, range(X.shape[0])] = 1

        # Compute gradients using backpropagation
        gradients_weights = []
        gradients_biases = []

        delta = activations[-1] - y_one_hot
        for i in reversed(range(len(self.layers) - 1)):
            gradients_weights.append(np.dot(delta, activations[i].T) / X.shape[0])
            gradients_biases.append(np.sum(delta, axis=1, keepdims=True) / X.shape[0])
            if i > 0:
                delta = np.dot(self.weights[i].T, delta) * activations[i] * (1 - activations[i])

        # Reverse the gradients lists to match the order of layers
        gradients_weights.reverse()
        gradients_biases.reverse()

        return gradients_weights, gradients_biases

    def update_weights(self, gradients_weights, gradients_biases):
        # Update weights and biases using gradient descent
        for i in range(len(self.weights)):
            self.weights[i] -= self.learning_rate * gradients_weights[i]
            self.biases[i] -= self.learning_rate * gradients_biases[i]

    def train(self, X, y, epochs):
        for epoch in range(epochs):
            activations = self.forward_propagation(X)
            gradients_weights, gradients_biases = self.backward_propagation(X, y, activations)
            self.update_weights(gradients_weights, gradients_biases)

            if epoch % 100 == 0:
                loss = -np.sum(np.log(activations[-1][y, range(X.shape[0])])) / X.shape[0]
                print(f"Epoch {epoch}, Loss: {loss:.4f}")

    def predict(self, X):
        activations = self.forward_propagation(X)
        predictions = np.argmax(activations[-1], axis=0)
        return predictions

# Example usage:
if __name__ == '__main__':
    # Generate a toy dataset (example with XOR)
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([0, 1, 1, 0])

    # Create a neural network with 2 input neurons, 2 hidden neurons, and 2 output neurons
    nn = NeuralNetwork(layers=[2, 2, 2], learning_rate=0.1)

    # Train the neural network
    nn.train(X, y, epochs=1000)

    # Predict using the trained neural network
    predictions = nn.predict(X)
    print("Predictions:", predictions)
